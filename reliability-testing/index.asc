= Reliability Engineering
by Eugene Shelestovich

== {conceal}
[quote, Traditional SRE saying]
____
Hope is not a strategy.
____

== Who is SRE ?
* Created at Google in 2003 by Ben Treynor
* Now around 1500 engineers at Google
* Common at Twitter, Facebook, Dropbox, Amazon, GitHub, Atlassian and Fitbit
* A dedicated team of Software Engineers with Ops skills to continuously oversee the reliability of the products

== What does SRE do ?
* Maintains large production systems
* *Consults other teams about best practices*
* Improves tooling and automation
* Incidents and change management, blameless post-mortems
* Measures everything!
* *Identify systemic issues and bottlenecks*
* Quantifies failure and availability by setting SLOs, gives teams error budget
* Free agent, moving between projects and orgs

== What is Reliability ?
****
The probability of failure-free software operation for a specified period of time in a specified environment.
****

* Fault-tolerance
* High availability
* Scalability
* Security
* Performance

== {conseal}
Ability to keep operating even if a component, or multiple components, fail.

image::fault_tolerance.png[]

== {conseal}
The outage will be brief because it will not take long to redeploy the required component.

image::high_availability.png[]

== {conseal}
You are saving your business by ditching your compromised infrastructure.

image::disaster_recovery.png[]

== {conseal}
Ability of a system to handle a growing amount of work without a failure.

image::scalability_failure.jpg[]

== What can go wrong ?
[options="incremental"]
* Process crash (OOME, GC, Linux memory manager)
* Disk/memory errors (write failure, not enough space)
* Server dies (faulty hardware, power outage)
* Network glitch (split-brain, dropped packets, high latency, low bandwidth)
* Timing issues (clock skew, NTP misconfiguration)
* Byzantine failure (malicious packet, DDOS, etc)

== Hardware failures
* 1000 individual machine failures (2% per year)
* Thousands of hard drive failures (4% per year)
* 20 rack failures (40-80 machines instantly disappear, 1-6 hours to get back)
* 5 racks going wonky (10-20 machines seeing 50% packet loss)
* 12 router reloads (takes out DNS for a couple minutes)
* 8 network maintenances (30-minute random connectivity losses)
* Slow disks, bad memory, misconfigured machines, etc

== Law of Large Numbers
****
The more nodes and disks you have in your cluster, the more likely it is to lose data
****

Example:
* Super reliable servers
* MTBF of 30 years
* 10K of those
* Watch one server fail *every day*

== But Replication...
image:data_loss_probability.png[]
If a node has a 0.1% chance of dying, the graph shows that in a 10K-node cluster, the chance of permanently losing all three replicas of some piece of data is about 0.25% per day. Yes, you read that correctly: the risk of losing all three copies of some data is more than twice as great as the risk of losing a single node! What is the point of all this replication again?

So the probability of permanently losing data in a 10,000 node cluster is really 0.25% per day, that would mean a 60% chance of entirely losing some data in a year.

In an 10K-node cluster it’s almost certain that a few nodes are always dead at any given moment.

And we don't even account for correlated failures!

== Latency Amplification
Server with 1 ms avg but 1 sec P99 latency:

* Touch 1 of these - 1% of requests take ≥1 sec
* Touch 100 of these - [big]#63%# of requests take [big]#≥1 sec#

== {conseal}
image::page_abandonment.png[]

* Amazon: page load slowdown of just *1 second* costs *$1.6 billion* in sales each year +
* Google: by slowing search by *0.5 second* we could lose *8 million* views per day +
* *67%* of EU online consumers says site slowness is the top reason to abandon a purchase

== {conceal}
image::testing_pyramid.png[]

== {conseal}
.Simple Testing Can Prevent Most Critical Failures
****
https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-yuan.pdf
****

* 198 randomly sampled failures of Cassandra, HBase, HDFS, MapReduce, Redis
* Almost all (92%) are the result of incorrect handling of external errors
* Majority (77%) can be reproduced by a simple test
* Almost all (98%) are guaranteed to manifest on no more than 3 nodes
* Most failures (90%) require no more than 3 input events to get them to manifest
* 74% of them are deterministic – they are guaranteed to happen given the right input
* For a majority (84%), all of their triggering events are explicitly logged
* Logs are noisy though - average number of log messages printed by each failure is 824

== No retry on I/O error
Data loss incident on large production HBase cluster:

[source,java]
----
try {
  splitRegion(...);
} catch (Exception ex) { // flaky filesystem I/O error
  LOG.error("Split failed.");
  // TODO: retry split
}
----

== Over-catch exception
1000+ nodes HDFS cluster brought down by an over-catch:

[source,java]
----
try {
  namenode.registerDatanode();
} catch (Throwable t) { // network glitch on NameNode
  System.exit(-1);
}
----

== Missing shutdown
MapReduce job hang on ResourceManager restart. +
It took down a 4000+ nodes Hadoop production cluster:

[source,java]
----
try {
  ...
} catch (IOException e) {
  LOG.error(“Error event from RM: shutting down..”);
  // no actual shutdown implemented
}
----

== Byzantine failure
We had a bug in Kafka recently that lead to the server incorrectly interpreting a corrupt request as a corrupt log, and shutting itself down to avoid appending to a corrupt log. Single machine log corruption is the kind of thing that should happen due to a disk error, and bringing down the corrupt node is the right behavior—it shouldn’t happen on all the machines at the same time unless all the disks fail at once. But since this was due to corrupt requests, and since we had one client that sent corrupt requests, it was able to sequentially bring down all the servers. Oops.

== Clock bugs
A Linux bug which causes the system to crash after 208 days of uptime due to rounding error in time conversion logic. Since machines are commonly restarted sequentially this lead to a situation where a large percentage of machines went hard down one after another.
https://access.redhat.com/errata/RHBA-2012:0124

== GC bug
GC pressure in an ElasticSearch cluster can cause secondary nodes to declare a primary dead and to attempt a new election. GC pauses and high IO_WAIT times due to IO can cause split brain, write loss, and index corruption.
https://github.com/elastic/elasticsearch/issues/2488

== Cloud failures
Multiple network disruptions in EC2 caused a corrupted index for ElasticSearch and several hours of write loss in MongoDB cluster due to network partition and new leader election.
Microsoft Azure partitions caused RabbitMQ split-brain and loosing messages.

https://forums.aws.amazon.com/thread.jspa?messageID=454155
https://aws.amazon.com/message/65648/


== {conseal}

= Fault Injection

== Process Dies
[source,console]
----
eshelestovich:/$ kill -9 $(pidof node)
----

.Crash-Only Software (Cassandra)
[source,shell]
----
until ./start-myserver; do
    echo "Myserver crashed with code $?. Respawning.." >&2
    sleep 1
done
----

CAUTION: Make sure no data loss, no resource leaks, restarts correctly

== OOME
JVM, Linux memory manager
IntelliJ debugger + throw new OutOfMemoryError()

[source,console]
----
eshelestovich:/$ java -Xmx100M -jar ./myserver.jar
----

== Long GC
https://github.com/elastic/elasticsearch/issues/2488 +
GC pauses in ElasticSearch can cause split brain, write loss, and even index corruption.

[source,shell]
----
http-server -i false -c-1 -p 80 --cors
kill -SIGSTOP $(pidof node)
kill -SIGCONT $(pidof node)
curl --head --max-time 3 -L localhost:8000
----

CAUTION: Not only JVM, reported Golang pauses up to 10 sec

== Server Crash
Power outage, faulty hardware

[source,console]
----
init 6
shutdown -r now
reboot -f -r now # most violent
aws ec2 terminate-instances
----

Fail-over, fail-back

== Network Failure
Bad switch, buggy firmware, misconfigured firewall

[source,console]
----
sudo ifconfig lo0 down/up

sudo iptables -A INPUT -p icmp -j DROP
sudo iptables -A INPUT -p icmp -m statistic --mode random --probability 0.5 -j DROP
sudo iptables -A INPUT -p icmp -m statistic --mode nth --every 3 --packet 0 -j DROP
sudo iptables -D ... # revert
----

== Unreliable Network
Clients from New Zealand & China, flooded link, 3G/EDGE

[source,console]
----
sudo tc qdisc add dev ens4 root netem delay 500ms 20ms # with jitter
sudo tc qdisc add dev ens4 root netem duplicate 30%
sudo tc qdisc add dev ens4 root netem delay 250ms drop 10% corrupt 10% reorder 10% rate 750kbps # poor 3G/EDGE
sudo tc qdisc del dev ens4 root netem # revert
----

On OSX and BSD use `ipfw` or `pfctl`.

https://github.com/tylertreat/comcast

== Partial availability
Asymmetric link failure (A sees B, but not vice versa)
Service may not be available from certain locations (Pingdom, etc).
The Great Firewall of China, Roskomnadzor, misconfigured routing.

== Disk issues
.Read-only filesystem
[source,shell]
----
sudo su -
echo u > /proc/sysrq-trigger
----

.Disk full, write error
[source,shell]
----
echo 1 > /dev/full
dd if=/dev/zero of=/dev/full
----

.Read error
[source,shell]
----
cat /proc/self/mem
----

PetardFS, CharybdeFS (by ScyllaDB)

== Time
Leap second
Time drift in Docker https://github.com/docker/for-mac/issues/2076
NTP issues

== Automate It
* Python/Bash scripts
* HipChat/Slack/Telegram chat bots
* Failure Fridays @ PagerDuty, Game Days @ Stripe, DevAwesome Days @ Fitbit
* Security pentest bots
* Chaos Monkey, Chaos Gorilla, CHAMP from Netflix
* Chaos Cat from PagerDuty

== {conseal}
image::isolate_node.png[]

image::reboot_roulette.png[]

== Chaos Engineering
****
The discipline of experimenting on a distributed system
in order to build confidence in the system’s capability
to withstand turbulent conditions in production.
****

https://principlesofchaos.org/ +
https://medium.com/netflix-techblog/chaos-engineering-upgraded-878d341f15fa
https://www.pagerduty.com/blog/chaoscat-automating-fault-injection/

== Design for failure
* Find ways to be partially available. Better to give users limited functionality than an error page. 100 ms lookup over 99% of data is usually better than 1 sec lookup over 100% of your data.
* Rigorous monitoring, profiling, logging and tracing
* Learn to estimate your capacity. Back-of-the-envelope calculation. "Numbers Everyone Should Know".
* Put limits and timeouts on everything. That queue you have for batch processing items? Does it really need to be unbounded? When connecting to another service over the network, do you really need to block indefinitely? Do connections to your database need to remain forever, or would five minutes be so long that you’re probably better off killing it?

== {conseal}
* Implement backpressure throughout your system. HTTP 429.
* Retry, but with exponential back-off. Most failures are transient. Don't DDOS yourself.
* Adopt Feature Flags/Toggles. Safer deployments. Path to A/B testing.
* Use health checks and watchdog processes, re-route requests if needed
* Redundancy is a must. Watch out SPOF.
* Avoid distributed coordination. E.g. ID generation.
* If you can fit your problem in memory on a single machine - do it. StackOverflow sits on 2 machines.
* Judicious use of caching. Exploit data and time locality.
* Fail fast, fail often. Restart automatically. Optimize for MTTR.

== trash
[quote, Ben Treynor, VP and founder of Google SRE]
____
Fundamentally, it's what happens when you ask a software engineer to design an operations function.
____

[source,console]
----
grep NETEM /boot/config-$(uname -r)
sudo modprobe sch_netem
----

cat /proc/sys/kernel/sysrq
grep SYSRQ /boot/config-$(uname -r)
sudo sysctl -w kernel.sysrq=1



== {conseal}

= Questions
