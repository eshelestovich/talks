= Reliability Engineering
by Eugene Shelestovich

== What is Reliability ?
****
The probability of failure-free software operation for a specified period of time in a specified environment.
****

* Correctness
* Fault-tolerance
* High availability
* Scalability
* Security
* Performance

== {conseal}
.Fault Tolerance
****
Ability to keep operating even if a component, or multiple components, fail
****

image::fault_tolerance.png[]

== {conseal}
.High Availability
****
The outage will be brief because it will not take long to redeploy the required component
****

image::high_availability.png[]

== {conseal}
.Disaster Recovery
****
You are saving your business by ditching your compromised infrastructure
****

image::disaster_recovery.png[]

== {conseal}
.Scalability
****
Ability of a system to handle a growing amount of work without a failure
****

image::scalability_failure.jpg[]

== Who is SRE ?
* Created at Google in 2003 by Ben Treynor
* Now around 1500 engineers at Google
* Familiar concept in high-risk industries like Aviation and Health Care
* Now common at Twitter, Facebook, Dropbox, Amazon, GitHub, Atlassian and Fitbit
* A dedicated team of Software Engineers with Ops skills to continuously oversee the reliability of the products

== What does SRE do ?
* Maintains large production systems
* *Consults other teams about best practices*
* *Identify systemic issues and bottlenecks*
* Improves tooling and automation
* Incidents and change management, blameless post-mortems
* Measures everything!
* Quantifies failure and availability by setting SLOs, gives teams error budget
* Free agent, moving between projects and orgs

== {conceal}
[quote, Traditional SRE saying]
____
Hope is not a strategy
____

== Hardware failures
* 1000 individual machine failures (2% per year)
* Thousands of hard drive failures (4% per year)
* 20 rack failures (40-80 machines instantly disappear, 1-6 hours to get back)
* 5 racks going wonky (10-20 machines seeing 50% packet loss)
* 8 network maintenances (30-minute random connectivity losses)
* Slow disks, bad memory, misconfigured machines, etc...

== Law of Large Numbers
****
The more nodes and disks you have in your cluster, the more likely it is to lose data
****

Example:

* Super reliable servers
* MTBF of 30 years
* 10K of those
* Watch some server *fail every day*
* And we're not even talking about correlated failures!

== {conseal}
image:data_loss_probability.png[]

The probability of permanently losing *all 3 replicas* of some data in a *10K-nodes* cluster is *0.25% per day*, that means a *60%* chance of entirely losing the data *in a year*.

== {conseal}
image::cloud_downtime.png[]

CAUTION: Amazon's distributed infrastructure *5x* larger than competitors

== {conseal}
image::cloud_vendors.png[]

.2018 Lloyd's Research
****
A *3-day* failure of a top cloud services provider, would cost the US economy up to *$15B*, with most of the burden falling on small and medium-sized businesses
****

== Latency Amplification
Server with 1 ms avg but 1 sec P99 latency:

* Touch 1 of these - 1% of requests take ≥1 sec
* Touch 100 of these - [big]#63%# of requests take [big]#≥1 sec#

== {conseal}
image::page_abandonment.png[]

* Amazon: page load slowdown of just *1 second* costs *$1.6 billion* in sales each year +
* Google: by slowing search by *0.5 second* we could lose *8 million* views per day +
* *67%* of EU online consumers says site slowness is the top reason to abandon a purchase

== {conseal}

= Testing

== {conceal}
image::testing_pyramid.png[]

== {conseal}
image::sqlite.png[]

* 3 independently developed test harnesses
* 100% branch test coverage
* Millions and millions of test cases
* Out-of-memory tests
* I/O error tests
* Crash and power loss tests
* Fuzz tests
* Malformed database tests

== {conseal}
.Simple Testing Can Prevent Most Critical Failures
****
https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-yuan.pdf
****

* 200 randomly sampled failures of Cassandra, HBase, HDFS, MapReduce, Redis
* Almost all (92%) are the result of incorrect handling of external errors
* Majority (77%) can be reproduced by a *simple test*
* Almost all (98%) are guaranteed to manifest on no more than *3 nodes*
* Most failures (90%) require no more than *3 input events* to get them to manifest
* 74% of them are *deterministic* – they are guaranteed to happen given the right input
* For a majority (84%), all of their triggering events are explicitly *logged*
* Logs are noisy though - average number of log messages printed by each failure is 824

== No retry on I/O error
Data loss incident on large production HBase cluster:

[source,java]
----
try {
  splitRegion(...);
} catch (Exception ex) { // flaky filesystem I/O error
  LOG.error("Split attempt failed. Retrying...");
  // TODO: retry split
}
----

== Over-catch exception
1000+ nodes HDFS cluster brought down by an over-catch:

[source,java]
----
try {
  namenode.registerDatanode();
} catch (Throwable t) { // <1>
  System.exit(-1); // shouldn't happen anyways
}
----
<1> BOOM! Network glitch on NameNode

== Missing shutdown
MapReduce job hang on ResourceManager restart. +
It took down a 4000+ nodes Hadoop production cluster:

[source,java]
----
try {
  ...
} catch (IOException e) {
  LOG.error(“Error event from RM: shutting down..”);
  // TODO: somebody plz implement actual shutdown
}
----

== {conseal}

= Fault Injection

== What can go wrong ?
[options="incremental"]
* Process crash (OOM, GC, Linux memory manager)
* Disk/memory errors (write failure, not enough space)
* Server dies (faulty hardware, power outage)
* Network glitch (split-brain, dropped packets, high latency, low bandwidth)
* Timing issues (clock skew, NTP misconfiguration)
* Byzantine failure (malicious packet, DDOS, etc)

== Process Dies
Make sure no data loss, no resource leaks, restarts correctly

[source,console]
----
eshelestovich:/$ kill -9 $(pidof node)
----

.Crash-Only Software (Cassandra)
****
https://www.usenix.org/conference/hotos-ix/crash-only-software
****

== Server Crash
Power outage, faulty hardware

[source,console]
----
aws ec2 terminate-instances
init 6
shutdown -r now
reboot -f -r now
echo b/c > /proc/sysrq-trigger # most violent
----

CAUTION: Fail-over, fail-back

== SysRq Magic Keys
****
It is a ‘magical’ key combo you can hit which the kernel will respond to regardless of whatever else it is doing, unless it is completely locked up.
****

image::sysrq_commands.png[]

== OOM
.https://youtrack.jetbrains.com/issue/IDEA-148408
image::exception1.png[]

.Limit Heap
[source,console]
----
java -Xmx100M -jar ./myserver.jar
----

.Call Linux OOM-Killer
[source,console]
----
echo f > /proc/sysrq-trigger
----

== Long GC
https://github.com/elastic/elasticsearch/issues/2488 +
GC pauses in ElasticSearch can cause split brain, write loss, and even index corruption.

[source,console]
----
http-server -i false -c-1 -p 80 --cors
kill -SIGSTOP $(pidof node)
kill -SIGCONT $(pidof node)
curl --head --max-time 3 -L localhost:80
----

CAUTION: Not only JVM, reported Golang pauses up to 10 sec

== Network Failure
Bad switch, buggy firmware, misconfigured firewall

[source,console]
----
ifconfig lo0 down
iptables -A INPUT -p icmp -j DROP
iptables -A INPUT -p icmp -m statistic --mode random --probability 0.5 -j DROP
iptables -D ... # revert
----

== Unreliable Network
Clients from New Zealand & China, flooded link, 3G/EDGE

[source,console]
----
tc qdisc add dev ens4 root netem delay 500ms 20ms
tc qdisc add dev ens4 root netem duplicate 30%
tc qdisc add dev ens4 root netem delay 250ms drop 10% corrupt 10% reorder 10% rate 750kbps # poor 3G/EDGE
tc qdisc del dev ens4 root netem # revert
----

CAUTION: Since Linux 2.6. On OSX and BSD use `ipfw` or `pfctl`.

https://github.com/tylertreat/comcast

== Partial Availability
image::pingdom2.png[]

* Asymmetric link failure (A sees B, but not vice versa)
* Service not be available from certain locations
* The Great Firewall of China, Roskomnadzor, misconfigured routing, etc.

== Disk issues
.Read-only filesystem
[source,shell]
----
echo u > /proc/sysrq-trigger
----

.Disk full, write error
[source,shell]
----
echo 1 > /dev/full
dd if=/dev/zero of=/dev/full
----

.Read error
[source,shell]
----
cat /proc/self/mem
----

PetardFS, CharybdeFS (by ScyllaDB)

== Time
* Hardware wonkiness can push clocks days into the future or past
* Virtualization can case time drift (https://github.com/docker/for-mac/issues/2076)
* Misconfigured nodes may not have NTP enabled, or may not be able to reach upstream
* NTP servers can lie without quorum
* NTP corrects large time differentials by jumping the clock to the correct time
* Even when perfectly synchronized, POSIX time itself is not monotonic (leap second)
* JVM concurrent primitives may hang if time jumps backwards

CAUTION: For time-sensetive systems use logical clocks (monotonic counters) or 3-server NTP with GPS sync

== Chaos Engineering
****
The discipline of experimenting on a distributed system
in order to build confidence in the system’s capability
to withstand turbulent conditions in production.
****

https://principlesofchaos.org/ +
https://medium.com/netflix-techblog/chaos-engineering-upgraded-878d341f15fa
https://www.pagerduty.com/blog/chaoscat-automating-fault-injection/

== Integrate It
[options="incremental"]
* Python/Bash scripts
* HipChat/Slack/Telegram chat bots
* Security Pentest bots
* Chaos Monkey, Chaos Gorilla, CHAMP from Netflix, Chaos Cat from PagerDuty
* Failure Fridays @ PagerDuty, Game Days @ Stripe

== {conseal}
image::isolate_node.png[]

image::reboot_roulette.png[]

== Best Practices
[options="incremental"]
* Rigorous monitoring, profiling, logging and tracing
* Find ways to be partially available. Better to give users limited functionality than an error page
* Learn to estimate your capacity. Back-of-the-envelope calculation. "Numbers Everyone Should Know"
* Avoid distributed coordination. E.g. ID generation
* If you can fit your problem in memory on a single machine - do it. StackOverflow sits on 2 machines

== {conseal}
[options="incremental"]
* Put limits and timeouts on everything
* Implement backpressure throughout your system. HTTP 429
* Retry, but with exponential back-off. Most failures are transient. Don't DDOS yourself
* Use Circuit Breaker (Hystrix, Linkerd) for external services to avoid error storm
* Use healthchecks/probes to restart the service automatically if failed (K8S and Aurora have built-in)
* Fail fast, fail often. Optimize for MTTR
* Adopt Feature Flags/Toggles. Safer deployments. Quick rollbacks. Path to A/B testing

== Begin Today
[source,shell]
----
until ./start-myserver; do
    echo "Myserver crashed with code $?. Respawning.." >&2
    sleep 1
done
----

////
[quote, Ben Treynor, VP and founder of Google SRE]
____
Fundamentally, it's what happens when you ask a software engineer to design an operations function
____

[source,console]
----
grep NETEM /boot/config-$(uname -r)
sudo modprobe sch_netem
----

cat /proc/sys/kernel/sysrq
grep SYSRQ /boot/config-$(uname -r)
sudo sysctl -w kernel.sysrq=1

That queue you have for batch processing items? Does it really need to be unbounded? When connecting to another service over the network, do you really need to block indefinitely? Do connections to your database need to remain forever, or would five minutes be so long that you’re probably better off killing it?

https://bbossola.wordpress.com/2013/09/04/jvm-issue-concurrency-is-affected-by-changing-the-date-of-the-system/

large slides should appear incrementally
reduce time
More incremental bullet points

////

== {conseal}

= Questions
