= Reliability Engineering

== Contents
* Terms and definitions
* What can go wrong
* Why it's important
* How to test for failure
* Demo time
* Best practices

== {conseal}
.High Availability
****
Replace the failed component, so the outage is brief
****

image::high_availability.png[]

== {conseal}
.Fault Tolerance
****
Ability to keep operating even when some pieces fail
****

image::fault_tolerance.png[]

== {conseal}
.Disaster Recovery
****
Give up the compromised infrastructure to save the business
****

image::disaster_recovery.png[]

== Who is SRE ?
[options="incremental"]
* Software Engineers with Ops and QA skills to ensure product's reliability
* Responsible for availability, performance, automation, monitoring, change management, capacity planning, emergency response and best practices
* Created at Google in 2003 (1500+ engs now)
* Came from high-risk industries like Aviation and Health Care
* Now common at Twitter, Facebook, Amazon, GitHub and Fitbit

== {conseal}

= World is Unreliable

== What Can Go Wrong ?
[options="incremental"]
* Process crash (OOM, GC, Linux memory manager)
* Disk/memory errors (write failure, not enough space)
* Server dies (faulty hardware, power outage)
* Network glitch (split-brain, dropped packets, high latency, low bandwidth)
* Timing issues (clock skew, NTP misconfiguration)
* Byzantine failure (forged request, malicious packet, DDOS, etc)

== Hardware Failures
[options="incremental"]
* 1000 machines (2% per year)
* Thousands of hard drives (4% per year)
* 20 racks (40-80 machines instantly disappear, 1-6 hours downtime)
* 8 network maintenances (30-minute random connectivity losses)
* Slow disks, bad memory, misconfigured switches, etc.

== Law of Large Numbers
****
The more nodes and disks you have, the more likely it is to lose data!
****

Example:
[options="incremental"]
* Super reliable servers (MTBF 30 years)
* Take 10K of those
* Watch 1 server failure *every day*
* And we're not even talking about correlated failures!

== {conseal}
image:data_loss_probability.png[]

The probability of permanently losing *all 3 replicas* of some data in a *10K-nodes* cluster is *0.25% per day*, i.e. a *60%* chance of entirely losing the data *in a year*

== {conseal}
image::cloud_market_share.png[]

== {conseal}
image::cloud_downtime.png[]

== Downtime Cost
.2018 Lloyd's Research
****
*3-day* failure of any top cloud would cost the US economy up to *$15MM*, with the burden falling on small and medium-sized businesses
****

.Amazon Outage
****
Due to mishandling of that error condition, the server started returning empty responses to every request it received.

*$700M/day* or *$30M/hour* and potentially *$3.6MM* on Prime Day
****

== Latency Amplification

[options="incremental"]
* Server with *1 ms* avg but *1 sec* P99 latency
* Touch 1 of these - 1% of requests take ≥1 sec
* Touch 100 of these - [big]#63%# of requests take [big]#≥1 sec#
* Hedged requests & speculative retries might help

== {conseal}
image::page_abandonment.png[]

* Amazon: page load slowdown of just *1 sec* costs *$1.6MM* in sales per year +
* Google: by slowing search by *0.5 sec* we lose *8M* views per day +
* For *67%* of EU online consumers site slowness is the top reason to abandon a purchase

== {conseal}

= Testing for Failure

== {conseal}
image::outage_reasons.png[]

== {conseal}
image::sqlite.png[]

[options="incremental"]
* 3 independently developed test harnesses
* Millions of test cases
* 100% branch coverage
* Out-of-memory tests
* I/O error tests
* Crash and power loss tests
* Fuzz tests
* Malformed database tests

== {conseal}
.Simple Testing Can Prevent Most Critical Failures
****
https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-yuan.pdf
****

[options="incremental"]
* 200 randomly sampled failures of Cassandra, HBase, Hadoop & Redis
* 92% are the result of *incorrect handling* of external errors
* 77% can be reproduced by a *simple test*
* 98% are guaranteed to manifest on no more than *3 nodes*
* 90% require no more than *3 input events* to catch
* 74% of them are *deterministic*
* For 84%, all of their triggering events were *explicitly logged*
* Logs are *noisy* - 824 log messages on avg

== No retry on I/O error
Data loss incident on a large production HBase cluster:

[source,java]
----
try {
  splitRegion(...);
} catch (Exception ex) { // flaky filesystem I/O error
  LOG.error("Split attempt failed. Retrying...");
  // TODO: retry
}
----

== Over-catch Exception
1000+ nodes HDFS cluster crashed:

[source,java]
----
try {
  namenode.registerDatanode();
} catch (Throwable t) { // BOOM! Network glitch on NameNode
  System.exit(-1);
  //TODO: we should properly test it
}
----

== Missing Shutdown
MapReduce job hung on ResourceManager restart. +
It took down a 4000+ nodes Hadoop production cluster:

[source,java]
----
try {
  ...
} catch (IOException e) {
  LOG.error(“Error from ResourceManager: shutting down..”);
  // TODO: somebody plz implement actual shutdown!
}
----

== Fault Injection
image::disaster_girl.jpg[]

== Process Crash
Check for data loss, resource leaks, graceful shutdown

[source,console]
----
eshelestovich:/$ kill -9 $(pidof node)
----

.Crash-Only Software (Cassandra)
****
https://www.usenix.org/conference/hotos-ix/crash-only-software
****

== Server Crash
Power outage, faulty hardware, fail-over, fail-back

[source,console]
----
init 6
shutdown -r now
reboot -f -r now
echo c > /proc/sysrq-trigger # most violent
aws ec2 terminate-instances
----

== Out Of Memory
image::exception1.png[]

.Limit Heap
[source,console]
----
java -Xmx64M -jar ./myserver.jar
----

.Call Linux OOM-Killer
[source,console]
----
echo f > /proc/sysrq-trigger
----

== Unresponsiveness
Long GC, deadlock, pool starvation, causes split brain, write loss, db index corruption

[source,console]
----
python3 -m http.server 8080
curl --head --max-time 2 -L localhost:8080
kill -SIGSTOP $(pidof python)
kill -SIGCONT $(pidof python)
----

CAUTION: Not only a JVM issue!

== Network Failure
Bad switch, crappy firmware, misconfigured firewall

[source,console]
----
ifconfig lo0 down
iptables -A INPUT -p icmp -j DROP
----

CAUTION: Works on new connections only

== Unreliable Network
Clients from NZ or China, noisy link, 3G/EDGE

[source,console]
----
tc qdisc add dev ens4 root netem delay 300ms 20ms drop 30%
tc qdisc add dev ens4 root netem delay 200ms drop 10% corrupt 10% reorder 10% rate 750kbps # poor 3G/EDGE
tc qdisc del dev ens4 root netem # revert
----

CAUTION: since Linux 2.6. On OSX & BSD use `ipfw` or `pfctl`.

== Disk Issues
.Read-only filesystem
[source,shell]
----
echo u > /proc/sysrq-trigger
----

.Disk full, write error
[source,shell]
----
echo 1 > /dev/full
dd if=/dev/zero of=/dev/full
----

.Read error
[source,shell]
----
cat /proc/self/mem
----

PetardFS, CharybdeFS by ScyllaDB

== Best Practices
[options="incremental"]
* Sleep with the solution over night
* No ninja commits to prod, infra as code, db updates as code
* Run on your laptop first, Docker Compose
* "Eat Your Own Dog Food", "You build it, you run it"
* Go read 12factor.net
* Rigorous monitoring, logging, tracing, exceptions tracking (sentry.io). "Hope is not a strategy"
* Estimate your capacity. Back-of-the-envelope calculations. Load/stress tests
* Don't over-engineer. StackOverflow sits on 2 machines

== {conseal}
[options="incremental"]
* Fail fast, fail often. Optimize for MTTR, not MTBF
* Healthchecks/probes to restart the service automatically (K8S, Aurora)
* Retry, with jittered exponential back-off. Most failures are transient. Don't DDOS yourself
* Put limits and timeouts on everything: bounded queues, # of threads, cache size
* Backpressure & throttling. HTTP 429
* Circuit Breaker (Hystrix, resilience4j) and Service Mesh (Linkerd, Istio, Consul)
* Be partially available. Limited or stale functionality is better than an error
* Avoid distributed coordination. E.g. ID generators, global locks, leader election
* Adopt Feature Flags/Toggles. Safer releases. Quick rollbacks. Path to A/B testing
* Blue/Green, Canary rollout. Safer releases

== Chaos Engineering
[options="incremental"]
* Bash/Python scripts
* Slack/Telegram chat bots
* Security pentest bots
* Chaos Monkey, Chaos Gorilla, CHAMP @ Netflix, Chaos Cat @ PagerDuty
* Failure Fridays @ PagerDuty, Game Days @ Stripe, DevAwesome Days @ Fitbit

== {conseal}
image::isolate_node.png[]

image::reboot_roulette.png[]

== Begin Today
[source,shell]
----
#!/bin/bash

until ./start-myserver; do
    echo "Server crashed with code $?. Respawning..." >&2
    sleep 1
done
----

== {conseal}
image::questions.jpg[]

////
== {conseal}
image::hiring.jpg[]

== Time
[options="incremental"]
* Virtualization can cause time drift (https://github.com/docker/for-mac/issues/2076)
* Misconfigured nodes may not have NTP enabled or may not be able to reach quorum
* NTP corrects time gaps by jumping the clock
* Even when perfectly synchronized, POSIX time itself is not monotonic (leap second)
* JVM concurrent primitives may hang if time jumps backwards
* For time-sensetive scenarios use logical clocks (monotonic counters) or 3-nodes NTP with GPS sync enabled
* https://bbossola.wordpress.com/2013/09/04/jvm-issue-concurrency-is-affected-by-changing-the-date-of-the-system/


[quote, Ben Treynor, VP and founder of Google SRE]
____
Fundamentally, it's what happens when you ask a software engineer to design an operations function
____

[source,console]
----
grep NETEM /boot/config-$(uname -r)
sudo modprobe sch_netem
----

https://github.com/elastic/elasticsearch/issues/2488

cat /proc/sys/kernel/sysrq
grep SYSRQ /boot/config-$(uname -r)
sudo sysctl -w kernel.sysrq=1

more contrast font for code snippets
less acronims, more explanation for terms (MTTR, MTBF, Circuit Braker perhaps clarification picture)
video fallback for the practical part
recap in the end

That queue you have for batch processing items?
Does it really need to be unbounded?
When connecting to another service over the network, do you really need to block indefinitely?
Do connections to your database need to remain forever, or would five minutes be so long that you’re probably better off killing it?

https://github.com/tylertreat/comcast

ssh -C eshelestovich@35.234.79.209
ping 35.234.79.209 -c 10000

sudo tc qdisc add dev ens4 root netem delay 300ms 20ms drop 30%
sudo tc qdisc add dev ens4 root netem delay 200ms drop 10% corrupt 10% reorder 10% rate 750kbps
sudo tc qdisc add dev ens4 root netem duplicate 30%
sudo tc qdisc del dev ens4 root netem

////
