= Reliability Engineering
=============================
:author: Eugene Shelestovich
:email: <stoweesh@gmail.com>
:description: just a template file.
:revdate: 2018-12-16
:backend: deckjs
///////////////////////
  Themes that you can choose includes:
  web-2.0, swiss, neon, beamer
///////////////////////
:deckjs_theme: swiss
///////////////////////
  Transitions that you can choose includes:
  fade, horizontal-slide, vertical-slide
///////////////////////
:deckjs_transition: horizontal-slide
///////////////////////
  AsciiDoc use `source-highlight` as default highlighter.

  Styles available for pygment highlighter:
  monokai, manni, perldoc, borland, colorful, default, murphy, vs, trac,
  tango, fruity, autumn, bw, emacs, vim, pastie, friendly, native,

  Uncomment following two lines if you want to highlight your code
  with `Pygments`.
///////////////////////
:pygments:
:pygments_style: colorful
///////////////////////
  Uncomment following line if you want to scroll inside slides
  with {down,up} arrow keys.
///////////////////////
//:scrollable:
///////////////////////
  Uncomment following line if you want to link css and js file
  from outside instead of embedding them into the output file.
///////////////////////
//:linkcss:
///////////////////////
  Uncomment following line if you want to count each incremental
  bullet as a new slide
///////////////////////
//:count_nested:
=============================

== {conceal}
[quote, Traditional SRE saying]
____
Hope is not a strategy.
____

== Who is SRE ?
* [x] Created at Google around 2003 by Ben Treynor
* Now around 1500 engineers at Google are SREs
* Exists in Apple, Twitter, Facebook, Dropbox, Amazon, GitHub, Atlassian and Fitbit
* Dev teams want to release awesome new features to the masses, and see them take off in a big way. Ops teams want to make sure those features don’t break things.
* We dedicate a team of Software Engineers with Ops skills to continuously oversee the reliability of the product and call them SRE.

== {conceal}
[quote, Ben Treynor, VP and founder of Google SRE]
____
Fundamentally, it's what happens when you ask a software engineer to design an operations function.
____

== What does SRE do ?
* Maintains large production systems
* *Consults other teams about best practices*
* Improves tooling and automation
* Incidents and change management, blameless post-mortems
* Measures everything!
* Quantifies failure and availability by setting SLOs, gives teams error budget
* *Identify systemic issues and bottlenecks*
* *Embraces risk and encourages developers to move quickly by reducing the cost of failure*
* Free agent, moving between projects and orgs

== Reliability in software

CAUTION: The probability of failure-free software operation for a specified period of time in a specified environment.

* Fault-tolerance
* High availability
* Scalability
* Security
* Performance

== Example
Internet is far from stable, connections drop and reappear, packets collide and are lost, and all kinds of other things happen. However, it's pretty amazing how reliable it is given all the instability inherent in it!

== {conseal}
Ability to keep operating even if a component, or multiple components, fail.

image::fault_tolerance.png[]

== {conseal}
The outage will be brief because it will not take long to redeploy the required component.

image::high_availability.png[]

== {conseal}
You are saving your business by ditching your compromised infrastructure.

image::disaster_recovery.png[]

== {conseal}
Ability of a system to handle a growing amount of work without a failure.

image::scalability_failure.jpg[]

== What can go wrong
* Process crash (OOME, GC, Linux memory manager)
* Server dies (faulty hardware, power outage)
* Network glitch (split-brain, dropped packets, high latency, firewall)
* Timing issues (clock skew, NTP misconfiguration)
* Byzantine failure (malicious packet, DDOS, etc)

== Hardware failures
* 1000 individual machine failures (2% per year)
* thousands of hard drive failures (4% per year)
* 20 rack failures (40-80 machines instantly disappear, 1-6 hours to get back)
* 5 racks going wonky (10-20 machines seeing 50% packet loss)
* 12 router reloads (takes out DNS and external vips for a couple minutes)
* 8 network maintenances (30-minute random connectivity losses)
* slow disks, bad memory, misconfigured machines, etc

== Things will crash

CAUTION: The more nodes and disks you have in your cluster, the more likely it is to lose data.

[incremental="true"]
* Super reliable servers
* MTBF of 30 years
* 10K of those
* Watch one fail *per day*

== But replication...
image::data_loss_probability.png[]
If a node has a 0.1% chance of dying, the graph shows that in a 10K-node cluster, the chance of permanently losing all three replicas of some piece of data is about 0.25% per day. Yes, you read that correctly: the risk of losing all three copies of some data is more than twice as great as the risk of losing a single node! What is the point of all this replication again?

So the probability of permanently losing data in a 10,000 node cluster is really 0.25% per day, that would mean a 60% chance of entirely losing some data in a year.

In an 10K-node cluster it’s almost certain that a few nodes are always dead at any given moment.

And we don't even account for correlated failures!

== Latency amplification
* Server with 1 ms avg but 1 sec P99 latency
* Touch 1 of these: 1% of requests take ≥1 sec
* Touch 100 of these: 63% of requests take ≥1 sec

== {conseal}
image::page_abandonment.png[]

* Amazon calculated that a page load slowdown of just 1 second could cost it $1.6 billion in sales each year. +
* Google calculated that by slowing its search results by 0.5 second they could lose 8 million searches per day. +
* A survey of 2,500 online consumers in the EU found out that 67% of shoppers said that site slowness is the top reason they’d abandon a purchase.

== {conceal}
image::testing_pyramid.png[]

== Software failures

== {conseal}
image::drive_survival.png[]

== Byzantine failure example
We had a bug in Kafka recently that lead to the server incorrectly interpreting a corrupt request as a corrupt log, and shutting itself down to avoid appending to a corrupt log. Single machine log corruption is the kind of thing that should happen due to a disk error, and bringing down the corrupt node is the right behavior—it shouldn’t happen on all the machines at the same time unless all the disks fail at once. But since this was due to corrupt requests, and since we had one client that sent corrupt requests, it was able to sequentially bring down all the servers. Oops.

== Clock bugs
RHBA-2012:0124
a Linux bug which causes the system to crash after 208 days of uptime due to rounding error in clock logic. Since machines are commonly restarted sequentially this lead to a situation where a large percentage of machines went hard down one after another.

== GC bug
https://github.com/elastic/elasticsearch/issues/2488
GC pressure in an ElasticSearch cluster can cause secondary nodes to declare a primary dead and to attempt a new election. GC pauses and high IO_WAIT times due to IO can cause split brain, write loss, and index corruption.

== Cloud failures
https://forums.aws.amazon.com/thread.jspa?messageID=454155
Multiple network disruptions in EC2 caused a corrupted index for ElasticSearch and several hours of write loss in MongoDB cluster due to network partition and new leader election. VoltDB.
Microsoft Azure partitions caused RabbitMQ split-brain and loosing messages.


sudo ifconfig lo0 down/up
ip link set eth0 down/up
kill -SIGKILL $(pidof java); sleep $(( 5 * 60 )); kill -SIGCONT $(pidof java);
sleep $[ ( $RANDOM % 10 )  + 1 ]s
time curl --head google.by
ab -c 1 -n 10 -i http://www.google.by/
http-server -i false -p 8000

== Process crash

== OOME

== Long GC

== Server crash

== Network failure

== Packet drops

== High latency

== Partial availability

== Disk issues

== Time

== Automation
image::isolate_node.png[]

image::reboot_roulette.png[]

== Chaos Engineering

== Design for failure
* Find ways to be partially available. Better to give users limited functionality than an error page. 100 ms lookup over 99% of data is usually better than 1 sec lookup over 100% of your data.
* Rigorous Monitoring, profiling, logging and tracing
* Learn to estimate your capacity. Capacity planning regularly. Numbers Everyone Should Know
* Put limits and timeouts on everything. That queue you have for batch processing items? Does it really need to be unbounded? When connecting to another service over the network, do you really need to block indefinitely? Do connections to your database need to remain forever, or would five minutes be so long that you’re probably better off killing it?
* Adopt Feature flags are how infrastructure is rolled out. Safer deployments. Path to A/B testing.
* Retry, but with exponential back-off. Most failures are transient. Don't DDOS yourself though.
* Use supervisors and watchdog processes
* Add health checks, and use them to re-route requests
* Redundancy is more than just nice-to-have. Watch out SPOF.
* Prefer battle-tested tools over the “new hotness”
* Avoid distributed coordination. E.g. ID generation. Keep communication and consensus between those machines is kept to a minimum.
* If you can fit your problem in memory on a single machine - do it. A modern Computer can do more than you think they can. StackOverflow hosting 2 machines.
* Implement backpressure throughout your system. exponential back-offs, error codes (http 429),
* Judicious use of caching. Exploit data and time locality.
* Fail-fast, fail often. Restart automatically. Optimize for quick recovery and MTTR.
* Parallelism
